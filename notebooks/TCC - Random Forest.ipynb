{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Bibliotecas necessarias e carregamento dos dados"
   ]
  },
  {
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# %% [code]\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"../data/CompleteDataSet.csv\", header=None, skiprows=2).dropna(axis=0)\n",
    "# print(data.describe())\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Separando os dados entre os sensores"
   ]
  },
  {
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "ankle_data = data.iloc[:, 1:7]\n",
    "\n",
    "right_pocket_data = data.iloc[:, 8:14]\n",
    "\n",
    "belt_data = data.iloc[:, 15:21]\n",
    "\n",
    "neck_data = data.iloc[:, 22:28]\n",
    "\n",
    "wrist_data = data.iloc[:, 29:35]\n",
    "\n",
    "eeg_data = data.iloc[:, 36]\n",
    "\n",
    "total_data = pd.concat([ankle_data, right_pocket_data, belt_data, neck_data, wrist_data], axis=1)\n",
    "\n",
    "sensors = {\"Ankle\": ankle_data, \"Right Pocket\": right_pocket_data, \"Belt\": belt_data, \"Neck\": neck_data, \"Wrist\": wrist_data}\n",
    "           \n",
    "\n",
    "activity = data.iloc[:, -3]\n",
    "\n",
    "features = [\"Acelerometro X\", \"Acelerometro Y\", \"Acelerometro Z\", \"Giroscopio X\", \"Giroscopio Y\",\n",
    "            \"Giroscopio Z\"]\n",
    "\n",
    "sensors_features = list(np.array([[\"Ankle \" + feature for feature in features],\n",
    "                                  [\"Right Pocket \" + feature for feature in features],\n",
    "                                  [\"Belt \" + feature for feature in features],\n",
    "                                  [\"Neck \" + feature for feature in features],\n",
    "                                  [\"Wrist \" + feature for feature in features]]).flat)\n",
    "\n",
    "atividades = [\"Falling\\nforward\\nusing hands\", \"Falling\\nforward\\nusing knees\", \"Falling\\nbackwards\",\n",
    "              \"Falling\\nsideward\",\n",
    "              \"Falling sitting\\nin empty chair\", \"Walking\", \"Standing\", \"Sitting\", \"Picking up\\nan object\", \"Jumping\",\n",
    "              \"Laying\"]\n",
    "\n",
    "total_data.columns = sensors_features"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Plotando a matriz de correlação da totalidade dos dados e de cada sensor separadamente"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# corr = total_data.corr()\n",
    "\n",
    "# plt.figure(dpi=300, figsize=(20, 20))\n",
    "# plt.title(\"Matriz de correlação - Todos os sensores\", fontdict={'fontsize': 18})\n",
    "# sns.heatmap(corr.round(2), cmap=\"RdBu\", xticklabels=sensors_features, yticklabels=sensors_features, vmin=-1, vmax=1,\n",
    "#             linecolor=\"black\", linewidths=0.5)\n",
    "# # plt.xticks(rotation=45)\n",
    "# # plt.yticks(rotation=45)\n",
    "# plt.savefig(\"total_data.png\", dpi=300, format=\"png\")\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# for sensor in sensors:\n",
    "#     corr = sensors[sensor].corr()\n",
    "\n",
    "#     plt.figure(dpi=300, figsize=(15, 15))\n",
    "#     plt.title(sensor, fontdict={'fontsize': 18})\n",
    "#     sns.heatmap(corr, cmap=\"RdBu\", vmin=-1, vmax=1, xticklabels=features, yticklabels=features, annot=True, square=True)\n",
    "#     plt.savefig(sensor + \".png\", dpi=300)\n",
    "#     plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Função para reportar as melhores pontuações do modelo"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                results['mean_test_score'][candidate],\n",
    "                results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Treinamento utilizando o Random Forest\n",
    "* Utiliza a busca de parametros aleatória com cross-validation\n",
    "* 30% dos dados são usados para teste e 70% para o treinamento"
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, min_samples_split=2, min_samples_leaf=1, bootstrap=True)\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "# param_dist = {\"max_depth\": [3, None],\n",
    "#               \"max_features\": sp_randint(1, 6),\n",
    "#               \"min_samples_split\": sp_randint(2, 11),\n",
    "#               \"bootstrap\": [True, False],\n",
    "#               \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# # run randomized search\n",
    "# n_iter_search = 20\n",
    "# random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=n_iter_search, n_jobs=4, cv=5,\n",
    "#                                    iid=False)\n",
    "\n",
    "# use a full grid over all parameters\n",
    "param_grid = {\"min_samples_split\": [2],\n",
    "              \"bootstrap\": [True],\n",
    "             \"n_estimators\": [10]}\n",
    "\n",
    "# # run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, iid=False, n_jobs=4)\n",
    "\n",
    "for sensor in sensors:\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(sensors[sensor], activity, test_size=0.3, random_state=42)\n",
    "    \n",
    "#     start = time()\n",
    "#     random_search.fit(X_train, y_train)\n",
    "# #     clf.fit(X_train, y_train)\n",
    "#     print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "#           \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "\n",
    "#     report(random_search.cv_results_)\n",
    "\n",
    "#     y_pred = random_search.predict(X_test)\n",
    "\n",
    "# #     y_pred = clf.predict(X_test)\n",
    "\n",
    "    start = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"GridSearchCV for Random Forest took %.2f seconds for %d candidate parameter settings.\"\n",
    "          % (time() - start, len(grid_search.cv_results_['params'])))\n",
    "    report(grid_search.cv_results_)\n",
    "\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Acuracia:\", accuracy)\n",
    "    \n",
    "    print(\"Random Forest clf_report (output_dict=False)\")\n",
    "    \n",
    "    clf_report = classification_report(y_test, y_pred, output_dict=False, target_names=[a.replace(\"\\n\", \" \") for a in atividades])\n",
    "    \n",
    "    print(clf_report)\n",
    "    \n",
    "#     print(\"Random Forest clf_report (output_dict=True)\")\n",
    "    clf_report = classification_report(y_test, y_pred, output_dict=True, target_names=[a.replace(\"\\n\", \" \") for a in atividades])\n",
    "    \n",
    "#     print(clf_report)\n",
    "\n",
    "    clf_report = pd.DataFrame(data=clf_report).drop(\"support\", axis=0).transpose()\n",
    "    clf_report.to_csv(sensor + \"_classification_report_rf.csv\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    pd.DataFrame(data=cm, index=atividades, columns=atividades).to_csv(sensor + \"_cm_rf.csv\")\n",
    "    \n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    pd.DataFrame(data=cm_norm, index=atividades, columns=atividades).to_csv(sensor + \"_cm_rf.csv\")\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    \n",
    "    plt.figure(dpi=300, figsize=(15, 10))\n",
    "    plt.title(sensor)\n",
    "    sns.heatmap(cm, xticklabels=atividades, yticklabels=atividades, cmap=\"RdBu\", annot=True, fmt='d')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.savefig(sensor + \"_matriz_confusao_rf.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(dpi=300, figsize=(15, 10))\n",
    "    plt.title(\"Matriz Normalizada\")\n",
    "    sns.heatmap(cm_norm.round(3), xticklabels=atividades, yticklabels=atividades, cmap=\"RdBu\", annot=True)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.savefig(sensor + \"_matriz_confusao_norm_rf.png\", dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Treinamento utilizando o XGBoost\n",
    "* Utiliza a busca de parametros aleatória com cross-validation\n",
    "* 30% dos dados são usados para teste e 70% para o treinamento"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf = xgb.XGBClassifier()\n",
    "\n",
    "# use a full grid over all parameters\n",
    "param_grid = {'max_depth': [6, 10]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, iid=False, n_jobs=4)\n",
    "\n",
    "for sensor in sensors:\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(sensors[sensor], activity, test_size=0.3, random_state=42)\n",
    "\n",
    "    start = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"GridSearchCV for XGBOOST took %.2f seconds for %d candidate parameter settings.\"\n",
    "          % (time() - start, len(grid_search.cv_results_['params'])))\n",
    "    report(grid_search.cv_results_)\n",
    "\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Acuracia:\", accuracy)\n",
    "    \n",
    "    print(\"XGBOOST clf_report (output_dict=False)\")\n",
    "    \n",
    "    clf_report = classification_report(y_test, y_pred, output_dict=False, target_names=[a.replace(\"\\n\", \" \") for a in atividades])\n",
    "    \n",
    "    print(clf_report)\n",
    "    \n",
    "#     print(\"XGBOOST clf_report (output_dict=True)\")\n",
    "    clf_report = classification_report(y_test, y_pred, output_dict=True, target_names=[a.replace(\"\\n\", \" \") for a in atividades])\n",
    "    \n",
    "#     print(clf_report)\n",
    "\n",
    "    clf_report = pd.DataFrame(data=clf_report).drop(\"support\", axis=0).transpose()\n",
    "    clf_report.to_csv(sensor + \"_classification_report_xgb.csv\")\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    pd.DataFrame(data=cm, index=atividades, columns=atividades).to_csv(sensor + \"_cm_xgb.csv\")\n",
    "    \n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    pd.DataFrame(data=cm_norm, index=atividades, columns=atividades).to_csv(sensor + \"_cm_xgb.csv\")\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    \n",
    "    plt.figure(dpi=300, figsize=(15, 10))\n",
    "    plt.title(sensor)\n",
    "    sns.heatmap(cm, xticklabels=atividades, yticklabels=atividades, cmap=\"RdBu\", annot=True, fmt='d')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.savefig(sensor + \"_matriz_confusao_xgb.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(dpi=300, figsize=(15, 10))\n",
    "    plt.title(\"Matriz Normalizada\")\n",
    "    sns.heatmap(cm_norm.round(3), xticklabels=atividades, yticklabels=atividades, cmap=\"RdBu\", annot=True)\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.savefig(sensor + \"_matriz_confusao_norm_xgb.png\", dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "#     plt.figure(dpi=300)\n",
    "#     plt.title(sensor)\n",
    "#     xgb.plot_importance(grid_search)\n",
    "#     plt.savefig(sensor + \"_importance_xgb.png\", dpi=300)\n",
    "#     plt.show()\n",
    "    \n",
    "#     plt.figure(dpi=300)\n",
    "#     plt.title(sensor)\n",
    "#     xgb.plot_tree(grid_search, num_trees=2)\n",
    "#     plt.savefig(sensor + \"_tree_xgb.png\", dpi=300)\n",
    "#     plt.show()\n",
    "\n",
    "    \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}